---
title: "Introduction to Machine Learning in R with tidymodels"
author: "Keaton Wilson"
date: "5/22/2019"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rmarkdown)
```

# Introduction to Machine Learning in R with caret  
***  

## Part 1 - What is machine learning? What are the tenets, what is the basic workflow?  

### Discussion - two questions (5-minutes with the person sitting next to you - then we'll come together and discuss as a group) 
1. What is machine learning?  
2. How is it different than statistics?  

#### Some important things to know and think about:  
1. Prediction is usually more important than explanation  
2. Two major types of problems - regression and classification  
3. Splitting the data to prevent overfitting  


### Classifcation Problem - Wine varietal identifier  

Here is the scenario: we've been contacted by a famous vignter in Italy because she suspects that one of the prized varietals (a rare version of *Aglianicone* that her family has grown for 7 generations) from her vinyard has been stolen, and is being grown and sold to make competitively delicious wine in the United States. The competing winemaker claims that the varietal being grown in the US is from a closely related varietal from the same region, that he obtained legally.  

Our customer has hired us to develop an algorithm to determine the likelihood that this is the wine being sold by the competitor was made from the varietal grown on her farm. Unfortunately, we don't have fancy genomic data to work with, but she has provided us with chemical profiles of a bunch of different wines made from both her grapes and two varietals that the competitor claims to be working with. The owner of the competing US vinyard has graciously provided us with the same type of data from a bunch of his wines to make comparisons on - he's looking to clear his name (and probably doesn't also believe that an algorithm can predict whether or not a given wine comes from a certain regional varietal)   


### Part 2 - Examining the Data

```{r, message=FALSE, warning=FALSE}
# Getting libraries we need loaded
library(tidymodels)
library(tidyverse)

#Reading in the data from the github repo
wine_data = read_csv("https://tinyurl.com/y7xqe278")

#Overviews
glimpse(wine_data)
summary(wine_data)

#making varietal a factor
wine_data = wine_data %>%
  mutate(varietal = as.factor(varietal))

#Checking for NAs
sum(is.na(wine_data))

#Uh oh - conversation about missing data. 

wine_data = wine_data %>%
  drop_na()

#or we can deal with it in pre-processing.
```
Ok, so this looks good. We have our item we want to classify in column 1, and all of our features in the rest. For our varietal numbers, 1 and 2 are the local varietals not owned by our customer, but varietal 3 is her special grape. So we're looking for the presence of any wines made from varietal 3 in the test set. 

**What do we need to do before we jump into to trying to build some algorithms?**  

Preprocess! In particular, we need to center and scale the data. *caret* can do this for us.  

### Part 3 - Preprocessing

``` {r}
#Setting up the preprocessing algorithm
set.seed(42)

# changing varietal to a factor
wine_data = wine_data %>%
  mutate(varietal = factor(varietal))

#Train and test split
data_split = initial_split(wine_data, prop = 0.80, strata = 'varietal')
wine_train = training(data_split)
wine_test = testing(data_split)

#Let's start to build a recipe  
wine_rec = recipe(varietal ~ ., data = wine_train) %>%
  step_knnimpute(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_nzv(all_predictors())
```


# Part 4 - Model Specification, Testing and Tuning

There are a ton of classification models to choose from - when starting ML stuff, this can be a really daunting part of the thing.  Today, we're going to explore a couple of bread-and-butter models:  
1. k-nn - nearest neighbord classifier  
2. Naive Bayes  
3. Decision Trees  
4. Support Vector Machines  

I'm not going to go into the math of how all of these operate at all. It's the beyond the scope of this workshop, but here is a good overview: https://medium.com/@sifium/machine-learning-types-of-classification-9497bd4f2e14  

One thing that we need to talk about briefly is resampling - this is the method we're going to use to assess how 'good' a model is, without applying it to the test data. There are a couple of main ways to do this:  
1. bootstrapping - random sampling within the dataset with replacement. Pulling a bunch of subsets of the data and looking at how the model performs across these subsets.  
2. Repeated n-fold cross-validation - does a bunch of splitting into training and test data **within** the training set, and then averages accuracy or RMSE across all these little mini-sets. 

We're going to use the second type.  

```{r, warning=FALSE, message=FALSE}
# setting up the model
rf_mod = rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

# let's build a workflow that pairs the model with the preprocessing
wine_wflow = workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(wine_rec)

# Can inspect
wine_wflow

# Let's fit the model and simultanesouly run preprocessing
wine_fit = wine_wflow %>%
  fit(data = wine_train)

# Done! Let's yank out some information about the model
wine_fit %>%
  pull_workflow_fit()

# let's run it on the test data and see how accurate it is
wine_pred = predict(wine_fit, wine_test, type = c("prob")) %>% 
  bind_cols(wine_test) 

wine_pred %>%
  roc_curve(truth = varietal, .pred_1:.pred_3) %>%
  autoplot()

wine_pred %>%
  roc_auc(truth = varietal, .pred_1:.pred_3)
```

The models default to using accuracy as the score to determine how good they are. Accuracy is **the percentage of the predictions made by the model that are correct**.  

Kappa compares observed accuracy with expected accuracy (chance). Less misleading than simply accuracy. 

Let's compare models  

```{r}
results = resamples(list(knn = knn_model, bayes = bayes_model, cart = cart_model, svm = svm_model))
summary(results)
dotplot(results)
svm_model
```

We can also look at the relative amount of false negatives and false positives with a confusion matrix.  

```{r}
predictions = predict(svm_model, wine_train_pp)
confusionMatrix(predictions, wine_train_pp$varietal)
```

Not particularly informative, given the model did a 100% accurate job of predicting on the training-data, but you get the gist. 

### Part 5 - Using the model on out of sample test data.  

```{r}
wine_test_pp$pred = predict(svm_model, wine_test_pp)

wine_test_pp %>%
  select(varietal, pred) %>%
  print(n = 32)

confusionMatrix(wine_test_pp$pred, wine_test_pp$varietal)

#we can even look at that wine

wine_test_pp %>%
  filter(pred != varietal)
```
  
### Part 6 - Continuing Practice  

Some resources if you want to get better at this:  
1. Kaggle - an online community of data scientists - lots of cool datasets to play with, and competitions!  
2. https://kbroman.org/pkg_primer/pages/resources.html - great list of resources!  
3. Machine Learning with R - Brett Lantz. Great book!  
4. [Great article on different algorithm types](https://medium.freecodecamp.org/when-to-use-different-machine-learning-algorithms-a-simple-guide-ba615b19fb3b)  

